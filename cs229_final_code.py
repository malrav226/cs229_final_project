# -*- coding: utf-8 -*-
"""CS229_FINAL_CODE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c7WnzCHIeePxpdaDv2kXYqX_lAV1WSEq

# Install Necessary Packages / Import Libraries
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn import model_selection, preprocessing
from tensorflow.keras import models, layers, optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers
from sklearn.utils import compute_class_weight
from sklearn.preprocessing import LabelEncoder

"""# Data Preprocessing

1. Get rid of rows and columns with high % of missingness
2. Create new outcomes of interest *(any_event, cause_of_death*)
3. Split into training and testing
4. Impute Missing Values with KNN imputation (5 nearest neighbors)
"""

myocardial_infarction_complications = fetch_ucirepo(id=579)

# data (as pandas dataframes)
X = myocardial_infarction_complications.data.features
y = myocardial_infarction_complications.data.targets

# address Missingness in Rows and Columns

# first, find percentage of missing values
missing_percentage = (X.isna().sum() / len(X)) * 100

# list of columns with over 30% missing values
columns_to_remove = missing_percentage[missing_percentage > 30].index
X = X.drop(columns = columns_to_remove)

# also remove rows with >20% missingess
missing_percentage_per_row = (X.isna().sum(axis=1) / X.shape[1]) * 100
rows_to_remove = missing_percentage_per_row[missing_percentage_per_row > 20].index

# remove the identified rows from both X and Y
X = X.drop(index=rows_to_remove)
y = y.drop(index=rows_to_remove)

# create binary outcome, any_event, which takes on a value of "1"
# if any of the 11 post-hospitilization complications occurs

events = ['FIBR_PREDS', 'PREDS_TAH', 'JELUD_TAH', 'FIBR_JELUD', 'A_V_BLOK',
       'OTEK_LANC', 'RAZRIV', 'DRESSLER', 'ZSN', 'REC_IM', 'P_IM_STEN']
y['any_event'] = y[events].any(axis=1).astype(int)


# consolidate the 8-class outcome, LET_IS, into more manageable groups
# we will try consolidating into 4 groups

def map_to_category(cause):
    if cause in [1, 3, 4]:
        return 'cardiac_complications'
    elif cause in [2, 5]:
        return 'pulmonary_complications'
    elif cause in [6, 7]:
        return 'cardiac_arrest'
    else:
        return 'alive'

y['cause_of_death'] = y['LET_IS'].apply(map_to_category)

# label encode cause_of_death: Useful for Multi-Class XGB Later On
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y['cause_of_death'])
y['cause_of_death_encoded'] = y_encoded

mapped_labels = {label: category for label, category in enumerate(label_encoder.classes_)}

# Display the mapping
print(mapped_labels)

# split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)

# KNN imputation with 5 Nearest Neighbors for missing values
# Perform imputation on train and test sets separately to avoid leakage

def impute_values(X, n):
    imputer = KNNImputer(n_neighbors=n)
    imputed = imputer.fit_transform(X)
    return imputed

X_train_imputed = impute_values(X_train, 5)
X_test_imputed = impute_values(X_test, 5)

"""# Preliminary Statistics


1.   Examine the size of train/test data
2.   Examine the proportions of our various outcomes of interest in the training, testing, and overall data. This will inform which metrics to consider and if we need to attempt to rebalance the data at all.
3. PCA visualizations to assess separabilty.




"""

# Size of training and test data
print(len(X_train))
print(len(X_test))

# number of features
print(X_train.shape[1])

# checking balance in the 'any_event' outcome

prop_any_event = sum(y['any_event'])/len(y)
print(f'Proportion of patients where any event occurs: {prop_any_event}')

# checking counts of LET_IS and cause_of_death
print("Counts of Each Cause of Mortality (original groupings)")
print(y['LET_IS'].value_counts())
print("------------------------")
print("Counts of Each Cause of Mortality (4 groups)")
print(y['cause_of_death'].value_counts())

# checking proportion of LET_IS and cause_of_death
print("Proportions of Each Cause of Mortality (original groupings)")
print(y['LET_IS'].value_counts(normalize=True))
print("------------------------")
print("Proportions of Each Cause of Mortality (4 groups)")
print(y['cause_of_death'].value_counts(normalize=True))

"""We see above that cause of mortality outcomes are much more imbalanced in the data, with a heavy bias towards the alive class (84% of all patients remaining alive) regardless of the way this outcome is grouped. This suggests we will need to rebalance for the multi-class setting.

Note that labels are:


*   0: Alive
*   1: Cardiogenic shock
*   2: Pulmonary edema
*   3: Myocardial rupture
*   4: Progress of congestive heart failure
*   5: Thromboembolism
*   6: Asystole
*   7: Ventricular fibrillation.
"""

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_imputed)
explained_var_ratio = pca.explained_variance_ratio_
total_explained_variance = np.sum(explained_var_ratio)
print(round(total_explained_variance * 100), 2)

plt.figure(figsize=(8, 6))

# Plotting points with label 0
plt.scatter(X_pca[y_train['any_event'] == 0, 0], X_pca[y_train['any_event']== 0, 1], label='Class 0', alpha=0.7)

# Plotting points with label 1
plt.scatter(X_pca[y_train['any_event'] == 1, 0], X_pca[y_train['any_event'] == 1, 1], label='Class 1', alpha=0.7)

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Data with Event/No Event Outcome')
plt.legend()
plt.grid(True)
plt.show()

# we see that binary outcome is not super linearly separable
# don't include in report

"""# Binary Classification: Any Event vs. No Event

## Logistic Regression with Lasso
"""

lr_model = LogisticRegression(penalty = "l1", solver='liblinear')
lr_model.fit(X_train_imputed, y_train['any_event'])


# make predictions on test data
y_pred_lr = lr_model.predict(X_test_imputed)
accuracy = accuracy_score(y_test['any_event'], y_pred_lr)
print(accuracy)
confusion_matrix(y_test['any_event'], y_pred_lr)

# make predictions on training data to assess overfitting, check accuracy
y_pred_lr_train = lr_model.predict(X_train_imputed)
accuracy_tr = accuracy_score(y_train['any_event'], y_pred_lr_train)
print(accuracy_tr)

print(classification_report(y_test['any_event'], y_pred_lr))

# figure out which points we are misclassifyibg
lr_misclassifications = (y_pred_lr != y_test['any_event'])

"""## XGBoost

We will use 5-fold cross validation to tune various hyperparameters.

Note: XGB handles missing values, so we do not need to use our imputed train/test data.
"""

#XGB can handle missing values on its own, don't need to worry about using imputed features


param_grid = {
    'learning_rate': [0.01, 0.1],
    'n_estimators': [100, 300],
    'max_depth': [5, 10, 30],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1.0] # important to tune for high dimensional, smaller datasets
}

xgb_model = xgb.XGBClassifier()

# 5fold CV
# resource: https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d
grid_search = GridSearchCV(estimator=xgb_model, scoring='accuracy', param_grid=param_grid,
                           cv=5, verbose=1)
grid_search.fit(X_train, y_train['any_event'])

best_params = grid_search.best_params_
print(best_params)

best_xgb_model = xgb.XGBClassifier(**best_params)
best_xgb_model.fit(X_train, y_train['any_event'])

# test
y_pred_xgb = best_xgb_model.predict(X_test)
accuracy = accuracy_score(y_test['any_event'], y_pred_xgb)
print("Accuracy on Test Set with Best Model:", accuracy)

# find training accuracy
y_pred_xgb_train = best_xgb_model.predict(X_train)
accuracy_tr = accuracy_score(y_train['any_event'], y_pred_xgb_train)
print("Accuracy on Training Set with Best Model:", accuracy_tr)

print(classification_report(y_test['any_event'], y_pred_xgb))

best_params

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming you have the predicted and true labels
# Replace predicted_labels and true_labels with your actual predicted and true labels
# Here is a sample confusion matrix creation using sklearn's confusion_matrix
# Replace this with your own confusion matrix if you already have it
conf_matrix = confusion_matrix(y_test['any_event'], y_pred_xgb)

conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted No Complication', 'Predicted Complication'],
                              index=['True No Complication', 'True Complication'])

print("Confusion Matrix:")
print(conf_matrix_df)

"""## Neural Network

"""

from keras import models
from keras import layers
from keras import regularizers
from keras import optimizers
from keras.callbacks import EarlyStopping

# called within CV loop
def create_model(hidden_units, activation, learning_rate, epochs, batch_size,
                 dropout_rate, num_layers, l2_penalty):
    model = models.Sequential()

    # input
    model.add(layers.Dense(input_dim=X_train.shape[1], units=hidden_units, activation=activation))

    # variable number of hidden layers
    # dropout in each layer
    for _ in range(num_layers):
        model.add(layers.Dense(units=hidden_units, activation=activation,
                               kernel_regularizer=regularizers.l2(l2_penalty)))
        model.add(layers.Dropout(dropout_rate))

    # outer activation layer
    model.add(layers.Dense(units=1, activation='sigmoid'))

    # compile model
    model.compile(loss='binary_crossentropy',
                  optimizer=optimizers.Adam(learning_rate=learning_rate),
                  metrics=['accuracy'])

    return model

#  hyperparameter lists
# will result in lots of combos! only run once if possible
import itertools

hidden_units_list = [50, 75]
activation_list = ['relu', 'tanh']
learning_rate_list = [0.01, 0.1]
epochs_list = [200]
dropout_rate_list = [0, 0.1, 0.3]
l2_penalty_list = [0.01, 0.1]
num_layers_list = [1, 3]
batch_size_list = [32]

hyperparameters_list = list(itertools.product(
    hidden_units_list,
    activation_list,
    learning_rate_list,
    epochs_list,
    dropout_rate_list,
    l2_penalty_list,
    num_layers_list,
    batch_size_list
))

from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=0)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

results = []

for hyperparams in hyperparameters_list:
    print(f"Testing Hyperparameters: {hyperparams}")

    # 5-fold cross validation
    # use 4 folds for HP tuning, 1 fold for validation and early stopping purposes
    fold_results = []
    fold_no = 1
    for train_index, val_index in kf.split(X_train, y_train):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train['any_event'].iloc[train_index], y_train['any_event'].iloc[val_index]

        # impute missing values within each fold, do not impute before splitting!
        # use 5 nearest neighbors as always
        X_train_fold_imputed = impute_values(X_train_fold, 5)
        X_val_fold_imputed = impute_values(X_val_fold, 5)

        # normalize inputs and get data into correct format for NN
        X_train_fold_imputed = preprocessing.normalize(X_train_fold_imputed)
        X_val_fold_imputed = preprocessing.normalize(X_val_fold_imputed)

        X_train_fold_imputed = np.asarray(X_train_fold_imputed).astype(np.float32)
        y_train_fold = np.asarray(y_train_fold).astype(np.float32)
        X_val_fold_imputed = np.asarray(X_val_fold_imputed).astype(np.float32)
        y_val_fold = np.asarray(y_val_fold).astype(np.float32)


        # create model with this set of HPs
        model = create_model(hidden_units=hyperparams[0], activation=hyperparams[1],
                             learning_rate=hyperparams[2], epochs=hyperparams[3],
                             dropout_rate=hyperparams[4], l2_penalty=hyperparams[5],
                             num_layers=hyperparams[6], batch_size=hyperparams[7])

        # train model-- perform early stopping on validation fold
        history = model.fit(X_train_fold_imputed, y_train_fold, epochs=hyperparams[3], batch_size=hyperparams[7],
                             validation_data=(X_val_fold_imputed, y_val_fold), verbose=0, callbacks=[early_stopping])
        #history = model.fit(X_train_fold_imputed, y_train_fold, epochs=hyperparams[3], batch_size=hyperparams[7],
                            #validation_data=(X_val_fold_imputed, y_val_fold), verbose=0)

        # evaluate the model on the validation fold
        val_loss, val_accuracy = model.evaluate(X_val_fold_imputed, y_val_fold)

        fold_results.append(val_accuracy)
        fold_no += 1

    # average accuray on validation set accross the 5 folds
    avg_val_accuracy = np.mean(fold_results)

    # store accuray for this set of hyperparamters-- use to figure out the best one after
    result = {
        'hidden_units': hyperparams[0],
        'activation': hyperparams[1],
        'learning_rate': hyperparams[2],
        'epochs': hyperparams[3],
        'dropout_rate': hyperparams[4],
        'l2_penalty': hyperparams[5],
        'num_layers': hyperparams[6],
        'batch_size': hyperparams[7],
        'avg_val_accuracy': avg_val_accuracy
        # Add other hyperparameters and results as needed
    }
    results.append(result)


results_df = pd.DataFrame(results)

# find the best hyperparameters in terms of highest validation accuracy
best_result = results_df.sort_values(by = 'avg_val_accuracy', ascending = False).iloc[0]

#loc[results_df['avg_val_accuracy'].idxmax()] -- GET RID OF THIS
# 50	tanh	0.01	200	0.3	0.01	1	32

best_hyperparameters = {
    'hidden_units': best_result['hidden_units'],
    'activation': best_result['activation'],
    'learning_rate': best_result['learning_rate'],
    'epochs': best_result['epochs'],
    'dropout_rate': best_result['dropout_rate'],
    'l2_penalty': best_result['l2_penalty'],
    'num_layers': best_result['num_layers'],
    'batch_size': best_result['batch_size']
}

# retrain with best parameters on entire training data
best_model_nn = create_model(hidden_units=best_hyperparameters['hidden_units'], activation=best_hyperparameters['activation'],
                             learning_rate=best_hyperparameters['learning_rate'], epochs=best_hyperparameters['epochs'],
                             dropout_rate=best_hyperparameters['dropout_rate'], l2_penalty=best_hyperparameters['l2_penalty'],
                             num_layers=best_hyperparameters['num_layers'], batch_size=best_hyperparameters['batch_size'])


# we already have the full imputed training set-- need to normalize inputs/format correctly
X_train_imputed_nn = preprocessing.normalize(X_train_imputed)
X_train_imputed_nn = np.asarray(X_train_imputed_nn).astype(np.float32)
y_train_nn = np.asarray(y_train['any_event']).astype(np.float32)

best_model_nn.fit(X_train_imputed_nn, y_train_nn, epochs=best_hyperparameters['epochs'],
               batch_size=best_hyperparameters['batch_size'], verbose=0)


# evaluate the best model on test set-- again, need to normalize inputs/format correctly
X_test_imputed_nn = preprocessing.normalize(X_test_imputed)
y_test_nn = np.asarray(y_test['any_event']).astype(np.float32)
test_loss, test_accuracy = best_model_nn.evaluate(X_test_imputed_nn, y_test_nn)
print(test_accuracy)


# evaluate the best model on training set to check for overfitting
training_loss, training_accuracy = best_model_nn.evaluate(X_train_imputed_nn, y_train_nn)
print(training_accuracy)

best_result

"""# Stacking

Resource: https://medium.com/@ab.rcb32/stacking-ensemble-learning-beginners-guide-c02b9c7352ac

Want to examine if ensembling predictions from XGBoost and Logistic Regression will improve accuracy.
[Not included in report]

"""

# split training data into training and validation
# do imputation within training and validation sets

x_train_stack, x_val_stack, y_train_stack, y_val_stack = train_test_split(X_train, y_train, test_size = 0.5, random_state = 10)
X_train_imputed_stack = impute_values(x_train_stack, 5)
X_val_imputed_stack = impute_values(x_val_stack, 5)

# train the two classifiers- LR and XGB- on smaller training subset
# again, we don't need imputation for XGB
lr_model_for_stacking = LogisticRegression(penalty = "l1", solver='liblinear')
lr_model_for_stacking.fit(X_train_imputed_stack, y_train_stack['any_event'])

xgb_model_for_stacking = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', **best_params)
xgb_model_for_stacking.fit(x_train_stack, y_train_stack['any_event'])

# make predictions on validation data
lr_val_preds = lr_model_for_stacking.predict_proba(X_val_imputed_stack)[:, 1]
xgb_val_preds = xgb_model_for_stacking.predict_proba(x_val_stack)[:, 1]
#lr_val_preds = lr_model_for_stacking.predict(X_val_imputed_stack)
#xgb_val_preds = xgb_model_for_stacking.predict(X_val_imputed_stack)

# make predictions on test data
lr_test_preds = lr_model.predict_proba(X_test_imputed)[:, 1]
xgb_test_preds = best_xgb_model.predict_proba(X_test)[:, 1]

# create new "training" set, which is the stacked predictions on val set
train_stack = np.column_stack((lr_val_preds, xgb_val_preds))

# create new "test" set from stacked predictions on test set
test_stack = np.column_stack((lr_test_preds, xgb_test_preds))

# fit final model (LR) and predict on test set
meta_model = LogisticRegression()
meta_model.fit(train_stack, y_val_stack['any_event'])

meta_predictions = meta_model.predict(test_stack)
stacking_accuracy = accuracy_score(y_test['any_event'], meta_predictions)
print(stacking_accuracy)
confusion_matrix(y_test['any_event'], meta_predictions)

print(classification_report(y_test['any_event'], meta_predictions))

"""# MultiClass Classification: Predicting cause of mortality

## Multiclass Logistic Regression (with 8 groups)

Start with all 8 outcomes, just to get a sense of how we perform.
"""

model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty = 'l1')
model.fit(X_train_imputed, y_train['LET_IS'])
y_pred = model.predict(X_test_imputed)
accuracy = accuracy_score(y_test['LET_IS'], y_pred)
report = classification_report(y_test['LET_IS'], y_pred)
print(report)

"""We see high accuracies, as expected, but very poor performance in other groups. We will try consolidating our classes, as well as applying different reweighting strategies, to see if this helps things.

## MultiClass Logistic Regression with Consolidated Groups

We will do 5-fold cross validation with different reweighting strategies to understand best how to address the class imbalance.

Our reweighting strategies are: none, balanced (inverse to class proportions), inverse squared, and exponentiated inverse.

Note that we will be using StratifiedKFold, rather than KFold, to ensure that each training/validation split has each class represented.
"""

from sklearn.metrics import f1_score
from sklearn.utils.class_weight import compute_class_weight
reweighting_strategies = ['none', 'balanced', 'square_inverse', 'exp_inverse']
LR_reweighting_results = []


skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

for reweighting_strategy in reweighting_strategies:
    fold_weighted_f1s = []
    fold_macro_f1s = []
    fold_macro_f1s_excluding_alive = []
    fold_no = 1
    for train_index, val_index in skf.split(X_train, y_train['cause_of_death']):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train['cause_of_death'].iloc[train_index], y_train['cause_of_death'].iloc[val_index]

        # impute missing values within each fold, do not impute before splitting!
        # use 5 nearest neighbors as always
        X_train_fold_imputed = impute_values(X_train_fold, 5)
        X_val_fold_imputed = impute_values(X_val_fold, 5)

        # compute different weight dictionaries
        class_weights_inverse = compute_class_weight('balanced', classes=np.unique(y_train_fold), y=y_train_fold)

        if reweighting_strategy == 'balanced':
          class_weight_dict = dict(zip(np.unique(y_train_fold), class_weights_inverse))

        elif reweighting_strategy == 'square_inverse':
          class_weight_dict = dict(zip(np.unique(y_train_fold), class_weights_inverse ** 2))

        elif reweighting_strategy == 'exp_inverse':
          class_weight_dict = dict(zip(np.unique(y_train_fold), np.exp(class_weights_inverse)))

        # train LR model and make predictions in same way
        if reweighting_strategy == 'none':
          model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty = 'l1')
        else:
          model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty = 'l1', class_weight=class_weight_dict)
        model.fit(X_train_fold_imputed, y_train_fold)
        y_pred = model.predict(X_val_fold_imputed)
        macro_f1 = f1_score(y_val_fold, y_pred, average = 'macro')
        weighted_f1 = f1_score(y_val_fold, y_pred, average = 'weighted')
        report = classification_report(y_val_fold, y_pred,  output_dict = True)
        macro_f1_excluding_alive = (report['cardiac_arrest']['f1-score'] + report['pulmonary_complications']['f1-score'] + report['cardiac_complications']['f1-score'])/3

        fold_weighted_f1s.append(weighted_f1)
        fold_macro_f1s.append(macro_f1)
        fold_macro_f1s_excluding_alive.append(macro_f1_excluding_alive)
        fold_no += 1

    avg_val_weighted_f1 = np.mean(fold_weighted_f1s)
    avg_val_macro_f1 = np.mean(fold_macro_f1s)
    avg_val_macro_f1_excluding_alive = np.mean(fold_macro_f1s_excluding_alive)

    # Store accuracy for this set of hyperparameters
    LR_reweighting_result = {
        'reweighting_strategy': reweighting_strategy,
        'avg_val_weighted_f1': avg_val_weighted_f1,
        'avg_val_macro_f1': avg_val_macro_f1,
        'avg_val_macro_f1_excluding_alive': avg_val_macro_f1_excluding_alive
    }
    LR_reweighting_results.append(LR_reweighting_result)

LR_reweighting_results_df = pd.DataFrame(LR_reweighting_results)

LR_reweighting_results_df

#  balanced parameter calculates the class weights inversely proportional to the class frequencies in the input data.
class_weights = compute_class_weight('balanced', classes=np.unique(y_train['cause_of_death']), y=y_train['cause_of_death'])
class_weight_dict = dict(zip(np.unique(y_train['cause_of_death']), class_weights))

# train LR model and make predictions in same way
model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty = 'l1', class_weight=class_weight_dict)
model.fit(X_train_imputed, y_train['cause_of_death'])
y_pred = model.predict(X_test_imputed)
accuracy = accuracy_score(y_test['cause_of_death'], y_pred)
report = classification_report(y_test['cause_of_death'], y_pred)
print(report)

print("CHECKING FOR OVERFITTING")


# print out report on training data to check for overfitting
model = LogisticRegression(multi_class='ovr', solver='liblinear', penalty = 'l1', class_weight=class_weight_dict)
model.fit(X_train_imputed, y_train['cause_of_death'])
y_pred_train = model.predict(X_train_imputed)
report = classification_report(y_train['cause_of_death'], y_pred_train)
print(report)

"""Using a "balanced" strategy is best. So, we will randomly upsample our data to be the size of the biggest class and then impute."""

from sklearn.utils import resample
from imblearn.over_sampling import RandomOverSampler, SMOTE

class_counts = y_train['cause_of_death'].value_counts()
minority_classes = class_counts[class_counts != class_counts.max()].index
columns = list(X_train.columns) + list(y_train.columns)
upsampled_data = pd.DataFrame(columns=columns)
combined_df = pd.DataFrame(np.hstack((X_train, np.array(y_train))), columns = columns)

for minority_class in minority_classes:
    # subset the minority class instances
    minority_data = combined_df[combined_df['cause_of_death'] == minority_class]

    # oversample the minority class to match the size of the majority class
    minority_upsampled = resample(minority_data, replace=True, n_samples=class_counts.max(), random_state=42)

    # Add the upsampled data to the DataFrame
    upsampled_data = pd.concat([upsampled_data, minority_upsampled])

# combine the upsampled data with the majority class data
balanced_data = pd.concat([combined_df[combined_df['cause_of_death'] == 'alive'], upsampled_data])

# separate the features and target variable from the upsampled data
X_train_upsampled = np.array(balanced_data.drop(y_train.columns, axis=1))
y_train_upsampled = balanced_data[y_train.columns]

# impute on upsampled training data
X_train_upsampled_imputed = impute_values(X_train_upsampled, 5)

"""# XGB Multiclass

We will use 5-fold cv as before, but within each fold we need to balance our data. Using stratified cv to ensure representative folds!

Note: do not need to impute for XGB
"""

from collections import defaultdict
from imblearn.over_sampling import RandomOverSampler, SMOTE
import itertools

from collections import defaultdict
from imblearn.over_sampling import RandomOverSampler, SMOTE
import itertools


param_grid_xgb_multiclass = {
    'learning_rate': [0.01, 0.1],
    'n_estimators': [100, 300],
    'max_depth': [5, 10, 30],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1.0] # important to tune for high dimensional, smaller datasets
}

param_combinations = list(itertools.product(*param_grid_xgb_multiclass.values()))
param_combinations_dicts = [{param: value for param, value in zip(param_grid_xgb_multiclass.keys(), combination)} for combination in param_combinations]
resampling_strategies = ['random']
XGB_multiclass_results = []
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)

for hp in param_combinations_dicts:
  for resampling_strategy in resampling_strategies: # don't need this, will just resample randomly
      print(hp)
      fold_weighted_f1s = []
      fold_macro_f1s = []
      fold_macro_f1s_excluding_alive = []
      fold_no = 1
      for train_index, val_index in skf.split(X_train, y_train['cause_of_death_encoded']):
          X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
          y_train_fold, y_val_fold = y_train['cause_of_death_encoded'].iloc[train_index], y_train['cause_of_death_encoded'].iloc[val_index]

          # no imputation necessary for XGBoost- but, we need to rebalance the training fold
          class_distribution = y_train_fold.value_counts().to_dict()
          majority_class_samples = max(class_distribution.values())

          # randomly oversample minority classes to match the size of "alive" class
          oversampler = RandomOverSampler(sampling_strategy={cls: majority_class_samples for cls in class_distribution.keys()})
          X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_fold, y_train_fold)


          # train XGB model
          xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class= 4, **hp)
          xgb_model.fit(X_train_resampled, y_train_resampled)
          y_pred = xgb_model.predict(X_val_fold)
          macro_f1 = f1_score(y_val_fold, y_pred, average = 'macro')
          weighted_f1 = f1_score(y_val_fold, y_pred, average = 'weighted')
          report = classification_report(y_val_fold, y_pred,  output_dict = True)
          macro_f1_excluding_alive = (report['1']['f1-score'] + report['2']['f1-score'] + report['3']['f1-score'])/3


          fold_weighted_f1s.append(weighted_f1)
          fold_macro_f1s.append(macro_f1)
          fold_macro_f1s_excluding_alive.append(macro_f1_excluding_alive) #
          fold_no += 1

      avg_val_weighted_f1 = np.mean(fold_weighted_f1s)
      avg_val_macro_f1 = np.mean(fold_macro_f1s)
      avg_val_macro_f1_excluding_alive =  np.mean(fold_macro_f1s_excluding_alive)

      print(avg_val_macro_f1)

      # Store performance for this set of hyperparameters
      XGB_multiclass_result = {
          'learning_rate': hp['learning_rate'],
          'n_estimators': hp['n_estimators'],
          'max_depth': hp['max_depth'],
          'subsample': hp['subsample'],
          'colsample_bytree': hp['colsample_bytree'],
          'avg_val_weighted_f1': avg_val_weighted_f1,
          'avg_val_macro_f1': avg_val_macro_f1,
          'avg_val_macro_f1_excluding_alive': avg_val_macro_f1_excluding_alive
      }
      XGB_multiclass_results.append(XGB_multiclass_result)

XGB_multiclass_results = pd.DataFrame(XGB_multiclass_results)

# find results of best model on test set
best_results = XGB_multiclass_results.sort_values(by = 'avg_val_macro_f1_excluding_alive', ascending = False).iloc[0]

# train on whole balanced training set
best_xgb_model_mc = xgb.XGBClassifier(objective='multi:softmax', num_class= 4,
                                      learning_rate = best_results['learning_rate'],
                                      n_estimators = int(best_results['n_estimators']),
                                      max_depth = int(best_results['max_depth']),
                                      subsample = best_results['subsample'],
                                      colsample_bytree = best_results['colsample_bytree'])
best_xgb_model_mc.fit(X_train_upsampled, y_train_upsampled['cause_of_death_encoded'])

# make predictions on test set
y_pred_xgb_mc = best_xgb_model_mc.predict(X_test)

# look at performance
accuracy = accuracy_score(y_test['cause_of_death_encoded'], y_pred_xgb_mc)
report = classification_report(y_test['cause_of_death_encoded'], y_pred_xgb_mc)

print(report)

# check for overfitting! make predictions on training data
y_pred_xgb_mc_train = best_xgb_model_mc.predict(X_train)

# look at performance
report = classification_report(y_train['cause_of_death_encoded'], y_pred_xgb_mc_train)
print(report)

# heatmap for XGB-- best performer
#{0: 'alive', 1: 'cardiac_arrest', 2: 'cardiac_complications', 3: 'pulmonary_complications'}

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test['cause_of_death_encoded'], y_pred_xgb_mc)
#labels = ['Alive', 'Cardiac Arrest', 'Cardiac Complications', 'Pulmonary Complications']

# Assuming conf_matrix is your confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues',
            xticklabels=[f'Class {i}' for i in range(conf_matrix.shape[0])],
            yticklabels=[f'Class {i}' for i in range(conf_matrix.shape[0])])

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""## Multiclass Neural Network

We will use 5-fold cv as before, but within each fold we need to balance our data. The function is slightly different as the outer layer uses a softmax activation. Using stratified cv to ensure representative folds!
"""

from keras import models
from keras import layers
from keras import regularizers
from keras import optimizers
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical

def create_model(hidden_units, activation, learning_rate, epochs, batch_size,
                 dropout_rate, num_layers, l2_penalty):
    model = models.Sequential()

    # input layer
    model.add(layers.Dense(units=hidden_units, activation=activation, input_dim=X_train.shape[1]))

    # variable number of hidden layers with dropout
    for _ in range(num_layers):
        model.add(layers.Dense(units=hidden_units, activation=activation,
                               kernel_regularizer=regularizers.l2(l2_penalty)))
        model.add(layers.Dropout(dropout_rate))

    # output layer for multi-class classification
    model.add(layers.Dense(units=4, activation='softmax'))

    # compile model
    model.compile(loss='categorical_crossentropy',
                  optimizer=optimizers.Adam(learning_rate=learning_rate),
                  metrics=['accuracy','AUC','Precision' ,'Recall'])

    return model

#  hyperparameter lists
# will result in lots of combos! only run once if possible
import itertools

hidden_units_list = [50, 75]
activation_list = ['relu', 'tanh']
learning_rate_list = [0.01, 0.1]
epochs_list = [200]
dropout_rate_list = [0, 0.1, 0.3]
l2_penalty_list = [0.01, 0.1]
num_layers_list = [1, 3]
batch_size_list = [32]

hyperparameters_list = list(itertools.product(
    hidden_units_list,
    activation_list,
    learning_rate_list,
    epochs_list,
    dropout_rate_list,
    l2_penalty_list,
    num_layers_list,
    batch_size_list
))

import random
import tensorflow as tf
np.random.seed(1)
random.seed(1)
tf.random.set_seed(1)


NN_multiclass_results = []
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


for hyperparams in hyperparameters_list:
    print(hyperparams)
    fold_weighted_f1s = []
    fold_macro_f1s = []
    fold_macro_f1s_excluding_alive = []
    fold_no = 1
    for train_index, val_index in skf.split(X_train, y_train['cause_of_death_encoded']):
        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_fold, y_val_fold = y_train['cause_of_death_encoded'].iloc[train_index], y_train['cause_of_death_encoded'].iloc[val_index]

        # Impute for NN
        X_train_fold_imputed = impute_values(X_train_fold, 5)
        X_val_fold_imputed = impute_values(X_val_fold, 5)

        # Rebalance the training folds-- randomly oversample to majority class
        class_distribution = y_train_fold.value_counts().to_dict()
        majority_class_samples = max(class_distribution.values())
        oversampler = RandomOverSampler(sampling_strategy={cls: majority_class_samples for cls in class_distribution.keys()})
        X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_fold_imputed, y_train_fold)

        # normalize inputs and get data into correct format
        X_train_fold = preprocessing.normalize(X_train_resampled)
        X_val_fold = preprocessing.normalize(X_val_fold_imputed)

        X_train_fold = np.asarray(X_train_fold).astype(np.float32)
        y_train_fold = np.asarray(y_train_resampled).astype(np.float32)
        X_val_fold = np.asarray(X_val_fold).astype(np.float32)
        y_val_fold = np.asarray(y_val_fold).astype(np.float32)

        # create and train NN model
        # create model with this set of HPs
        nn_model_mc = create_model(hidden_units=hyperparams[0], activation=hyperparams[1],
                                learning_rate=hyperparams[2], epochs=hyperparams[3],
                                dropout_rate=hyperparams[4], l2_penalty=hyperparams[5],
                                num_layers=hyperparams[6], batch_size=hyperparams[7])

        # train model-- perform early stopping using validation fold
        history = nn_model_mc.fit(X_train_fold, to_categorical(y_train_fold, num_classes=4), epochs=hyperparams[3], batch_size=hyperparams[7],
                                 validation_data=(X_val_fold, to_categorical(y_val_fold, num_classes=4)), verbose=0, callbacks=[early_stopping])
        #history = nn_model_mc.fit(X_train_fold, to_categorical(y_train_fold, num_classes=4), epochs=hyperparams[3], batch_size=hyperparams[7], verbose=1)

        # make predictions and evaluate
        y_pred = nn_model_mc.predict(X_val_fold).argmax(axis=1)
        macro_f1 = f1_score(y_val_fold, y_pred, average = 'macro')
        weighted_f1 = f1_score(y_val_fold, y_pred, average = 'weighted')
        report = classification_report(y_val_fold, y_pred,  output_dict = True)
        macro_f1_excluding_alive = (report['1.0']['f1-score'] + report['2.0']['f1-score'] + report['3.0']['f1-score'])/3


        fold_weighted_f1s.append(weighted_f1)
        fold_macro_f1s.append(macro_f1)
        fold_macro_f1s_excluding_alive.append(macro_f1_excluding_alive) #
        fold_no += 1


    avg_val_weighted_f1 = np.mean(fold_weighted_f1s)
    avg_val_macro_f1 = np.mean(fold_macro_f1s)
    avg_val_macro_f1_excluding_alive =  np.mean(fold_macro_f1s_excluding_alive)

    print(avg_val_macro_f1)

    # Store performance for this set of hyperparameters
    NN_multiclass_result = {
            'hidden_units': hyperparams[0],
            'activation': hyperparams[1],
            'learning_rate': hyperparams[2],
            'epochs': hyperparams[3],
            'dropout_rate': hyperparams[4],
            'l2_penalty': hyperparams[5],
            'num_layers': hyperparams[6],
            'batch_size': hyperparams[7],
            'avg_val_weighted_f1': avg_val_weighted_f1,
            'avg_val_macro_f1': avg_val_macro_f1,
            'avg_val_macro_f1_excluding_alive': avg_val_macro_f1_excluding_alive
        }
    NN_multiclass_results.append(NN_multiclass_result)

NN_multiclass_results = pd.DataFrame(NN_multiclass_results)

import random
import tensorflow as tf
np.random.seed(1)
random.seed(1)
tf.random.set_seed(1)

# Find best NN model and train entire balanced training set with these parameters
# find the best hyperparameters in terms of highest validation macro F1 (excluding alive)
best_result = NN_multiclass_results.sort_values(by = 'avg_val_macro_f1_excluding_alive', ascending = False).iloc[0]


best_hyperparameters = {
    'hidden_units': best_result['hidden_units'],
    'activation': best_result['activation'],
    'learning_rate': best_result['learning_rate'],
    'epochs': best_result['epochs'],
    'dropout_rate': best_result['dropout_rate'],
    'l2_penalty': best_result['l2_penalty'],
    'num_layers': best_result['num_layers'],
    'batch_size': best_result['batch_size']
}
#best_result = {'hidden_units': 50, 'activation': 'tanh', 'learning_rate': 0.01, 'epochs': 200, 'dropout_rate': 0, 'l2_penalty': 0.01, 'num_layers': 3, 'batch_size':32}


# retrain with best parameters on entire training data
best_model_nn = create_model(hidden_units=best_hyperparameters['hidden_units'], activation=best_hyperparameters['activation'],
                             learning_rate=best_hyperparameters['learning_rate'], epochs=best_hyperparameters['epochs'],
                             dropout_rate=best_hyperparameters['dropout_rate'], l2_penalty=best_hyperparameters['l2_penalty'],
                             num_layers=best_hyperparameters['num_layers'], batch_size=best_hyperparameters['batch_size'])


# train on balanced and imputed training dataset - we already have the full imputed training set-- need to normalize inputs/format correctly
X_train_imputed_nn = preprocessing.normalize(X_train_upsampled_imputed)
X_train_imputed_nn = np.asarray(X_train_imputed_nn).astype(np.float32)
y_train_nn = np.asarray(y_train_upsampled['cause_of_death_encoded']).astype(np.float32)

best_model_nn.fit(X_train_imputed_nn, to_categorical(y_train_nn, num_classes=4), epochs=best_hyperparameters['epochs'],
               batch_size=best_hyperparameters['batch_size'], verbose=0)


# evaluate the best model on test set-- again, need to normalize inputs/format correctly
X_test_imputed_nn = preprocessing.normalize(X_test_imputed)
y_test_nn = np.asarray(y_test['cause_of_death_encoded']).astype(np.float32)
y_pred = best_model_nn.predict(X_test_imputed_nn).argmax(axis=1)
report = classification_report(y_test_nn, y_pred)
print(report)


print("------Check for overfitting--------")
# need to find training error too!
y_pred_train = best_model_nn.predict(X_train_imputed_nn).argmax(axis=1)
report = classification_report(y_train_nn, y_pred_train)
print(report)